{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20af5502",
   "metadata": {},
   "source": [
    "# 04 - Test and Deploy Training Pipeline to Vertex Pipelines\n",
    "\n",
    "The purpose of this notebook is to test, deploy, and run the `TFX` pipeline on `Vertex Pipelines`. The notebook covers the following tasks:\n",
    "1. Run the tests locally.\n",
    "2. Run the pipeline using `Vertex Pipelines`\n",
    "3. Execute the pipeline deployment `CI/CD` steps using `Cloud Build`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe7e500",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba8ae0",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0bca67ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.2.0\n",
      "KFP Version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kfp\n",
    "import tfx\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)\n",
    "print(\"KFP Version:\", kfp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8e0896",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8834dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: aiops-industrialization\n",
      "Region: us-central1\n",
      "Bucket name: aiops-industrialization-bucket-ravi\n",
      "Service Account: 175728527123-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "PROJECT = 'aiops-industrialization' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET = 'aiops-industrialization-bucket-ravi'  # Change to your bucket name.\n",
    "SERVICE_ACCOUNT = \"175728527123-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "if PROJECT == \"\" or PROJECT is None or PROJECT == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == \"\" or BUCKET is None or BUCKET == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    BUCKET = PROJECT\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"Project ID:\", PROJECT)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ceddc",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a616bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'playground_us' # Change to your BQ dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_prep'\n",
    "\n",
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "CICD_IMAGE_NAME = 'cicd:latest'\n",
    "CICD_IMAGE_URI = f\"gcr.io/{PROJECT}/{CICD_IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "17ad33ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'src/raw_schema/.ipynb_checkpoints/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r src/raw_schema/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9c49b6",
   "metadata": {},
   "source": [
    "## 1. Run the CICD steps locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f73c16",
   "metadata": {},
   "source": [
    "### Set pipeline configurations for the local run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "48a1c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] =  MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BQ_LOCATION\"] = BQ_LOCATION\n",
    "os.environ[\"BQ_DATASET_NAME\"] = BQ_DATASET_NAME\n",
    "os.environ[\"BQ_TABLE_NAME\"] = BQ_TABLE_NAME\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"1000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"100\"\n",
    "os.environ[\"UPLOAD_MODEL\"] = \"0\"\n",
    "os.environ[\"ACCURACY_THRESHOLD\"] = \"0.1\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DirectRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19850efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: aiops-industrialization\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests\n",
      "ARTIFACT_STORE_URI: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-classifier-v01-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 1000\n",
      "TEST_LIMIT: 100\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/aiops-industrialization/chicago-taxi-tips:v01\n",
      "BEAM_RUNNER: DirectRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=aiops-industrialization', '--temp_location=gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=aiops-industrialization', '--temp_location=gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests/temp', '--region=us-central1', '--runner=DirectRunner']\n",
      "TRAINING_RUNNER: local\n",
      "AI_PLATFORM_TRAINING_ARGS: {'project': 'aiops-industrialization', 'region': 'us-central1', 'masterConfig': {'imageUri': 'gcr.io/aiops-industrialization/chicago-taxi-tips:v01'}}\n",
      "SERVING_RUNTIME: tf2-cpu.2-4\n",
      "SERVING_IMAGE_URI: gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-4:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DirectRunner', 'temporary_dir': 'gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests/temp', 'gcs_location': 'gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests/temp', 'project': 'aiops-industrialization', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c885e20",
   "metadata": {},
   "source": [
    "### Run unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test ./src/tests/datasource_utils_tests.py -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py.test src/tests/model_tests.py -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e2632",
   "metadata": {},
   "source": [
    "### Run e2e pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /home/jupyter/mlops-with-vertex-ai\n",
      "plugins: anyio-3.3.0\n",
      "collecting ... 2021-08-31 13:58:38.662547: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "collected 1 item                                                               \n",
      "\n",
      "src/tests/pipeline_deployment_tests.py upload_model: 0\n",
      "Pipeline e2e test artifacts stored in: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests\n",
      "ML metadata store is ready.\n",
      "F\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "______________________________ test_e2e_pipeline _______________________________\n",
      "\n",
      "    def test_e2e_pipeline():\n",
      "    \n",
      "        project = os.getenv(\"PROJECT\")\n",
      "        region = os.getenv(\"REGION\")\n",
      "        model_display_name = os.getenv(\"MODEL_DISPLAY_NAME\")\n",
      "        dataset_display_name = os.getenv(\"DATASET_DISPLAY_NAME\")\n",
      "        gcs_location = os.getenv(\"GCS_LOCATION\")\n",
      "        model_registry = os.getenv(\"MODEL_REGISTRY_URI\")\n",
      "        upload_model = os.getenv(\"UPLOAD_MODEL\")\n",
      "    \n",
      "        assert project, \"Environment variable PROJECT is None!\"\n",
      "        assert region, \"Environment variable REGION is None!\"\n",
      "        assert dataset_display_name, \"Environment variable DATASET_DISPLAY_NAME is None!\"\n",
      "        assert model_display_name, \"Environment variable MODEL_DISPLAY_NAME is None!\"\n",
      "        assert gcs_location, \"Environment variable GCS_LOCATION is None!\"\n",
      "        assert model_registry, \"Environment variable MODEL_REGISTRY_URI is None!\"\n",
      "    \n",
      "        logging.info(f\"upload_model: {upload_model}\")\n",
      "        if tf.io.gfile.exists(gcs_location):\n",
      "            tf.io.gfile.rmtree(gcs_location)\n",
      "        logging.info(f\"Pipeline e2e test artifacts stored in: {gcs_location}\")\n",
      "    \n",
      "        if tf.io.gfile.exists(MLMD_SQLLITE):\n",
      "            tf.io.gfile.remove(MLMD_SQLLITE)\n",
      "    \n",
      "        metadata_connection_config = metadata_store_pb2.ConnectionConfig()\n",
      "        metadata_connection_config.sqlite.filename_uri = MLMD_SQLLITE\n",
      "        metadata_connection_config.sqlite.connection_mode = 3\n",
      "        logging.info(\"ML metadata store is ready.\")\n",
      "    \n",
      "        pipeline_root = os.path.join(\n",
      "            config.ARTIFACT_STORE_URI,\n",
      "            config.PIPELINE_NAME,\n",
      "        )\n",
      "    \n",
      "        runner = LocalDagRunner()\n",
      "    \n",
      "        pipeline = training_pipeline.create_pipeline(\n",
      "            pipeline_root=pipeline_root,\n",
      "            num_epochs=NUM_EPOCHS,\n",
      "            batch_size=BATCH_SIZE,\n",
      "            learning_rate=LEARNING_RATE,\n",
      "            hidden_units=HIDDEN_UNITS,\n",
      ">           metadata_connection_config=metadata_connection_config,\n",
      "        )\n",
      "\n",
      "src/tests/pipeline_deployment_tests.py:82: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "pipeline_root = 'gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests/tfx_artifacts/chicago-taxi-tips-classifier-v01-train-pipeline'\n",
      "num_epochs = 1, batch_size = 512, learning_rate = 0.001\n",
      "hidden_units = '128,128'\n",
      "metadata_connection_config = sqlite {\n",
      "  filename_uri: \"mlmd.sqllite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "\n",
      "    def create_pipeline(\n",
      "        pipeline_root: str,\n",
      "        num_epochs: data_types.RuntimeParameter,\n",
      "        batch_size: data_types.RuntimeParameter,\n",
      "        learning_rate: data_types.RuntimeParameter,\n",
      "        hidden_units: data_types.RuntimeParameter,\n",
      "        metadata_connection_config: metadata_store_pb2.ConnectionConfig = None,\n",
      "    ):\n",
      "    \n",
      "        local_executor_spec = executor_spec.ExecutorClassSpec(\n",
      "            trainer_executor.GenericExecutor\n",
      "        )\n",
      "    \n",
      "        caip_executor_spec = executor_spec.ExecutorClassSpec(\n",
      "            ai_platform_trainer_executor.GenericExecutor\n",
      "        )\n",
      "    \n",
      "        # Hyperparameter generation.\n",
      "        hyperparams_gen = custom_components.hyperparameters_gen(\n",
      "            num_epochs=num_epochs,\n",
      "            batch_size=batch_size,\n",
      "            learning_rate=learning_rate,\n",
      "            hidden_units=hidden_units,\n",
      "        ).with_id(\"HyperparamsGen\")\n",
      "    \n",
      "        # Get train source query.\n",
      "        train_sql_query = datasource_utils.get_training_source_query(\n",
      "            config.PROJECT,\n",
      "            config.REGION,\n",
      "            config.DATASET_DISPLAY_NAME,\n",
      "            ml_use=\"UNASSIGNED\",\n",
      "            limit=int(config.TRAIN_LIMIT),\n",
      "        )\n",
      "    \n",
      "        train_output_config = example_gen_pb2.Output(\n",
      "            split_config=example_gen_pb2.SplitConfig(\n",
      "                splits=[\n",
      "                    example_gen_pb2.SplitConfig.Split(\n",
      "                        name=\"train\", hash_buckets=int(config.NUM_TRAIN_SPLITS)\n",
      "                    ),\n",
      "                    example_gen_pb2.SplitConfig.Split(\n",
      "                        name=\"eval\", hash_buckets=int(config.NUM_EVAL_SPLITS)\n",
      "                    ),\n",
      "                ]\n",
      "            )\n",
      "        )\n",
      "    \n",
      "        # Train example generation.\n",
      "        train_example_gen = BigQueryExampleGen(\n",
      "            query=train_sql_query,\n",
      "            output_config=train_output_config,\n",
      "        ).with_id(\"TrainDataGen\")\n",
      "    \n",
      "        # Get test source query.\n",
      "        test_sql_query = datasource_utils.get_training_source_query(\n",
      "            config.PROJECT,\n",
      "            config.REGION,\n",
      "            config.DATASET_DISPLAY_NAME,\n",
      "            ml_use=\"TEST\",\n",
      "            limit=int(config.TEST_LIMIT),\n",
      "        )\n",
      "    \n",
      "        test_output_config = example_gen_pb2.Output(\n",
      "            split_config=example_gen_pb2.SplitConfig(\n",
      "                splits=[\n",
      "                    example_gen_pb2.SplitConfig.Split(name=\"test\", hash_buckets=1),\n",
      "                ]\n",
      "            )\n",
      "        )\n",
      "    \n",
      "        # Test example generation.\n",
      "        test_example_gen = BigQueryExampleGen(\n",
      "            query=test_sql_query,\n",
      "            output_config=test_output_config,\n",
      "        ).with_id(\"TestDataGen\")\n",
      "    \n",
      "        # Schema importer.\n",
      "        schema_importer = Importer(\n",
      "            source_uri=RAW_SCHEMA_DIR,\n",
      "            artifact_type=tfx.types.standard_artifacts.Schema,\n",
      "        ).with_id(\"SchemaImporter\")\n",
      "    \n",
      "        # Statistics generation.\n",
      ">       statistics_gen = StatisticsGen(examples=train_example_gen.outputs.examples).with_id(\n",
      "            \"StatisticsGen\"\n",
      "        )\n",
      "E       AttributeError: 'dict' object has no attribute 'examples'\n",
      "\n",
      "src/tfx_pipelines/training_pipeline.py:146: AttributeError\n",
      "------------------------------ Captured log call -------------------------------\n",
      "INFO     root:pipeline_deployment_tests.py:56 upload_model: 0\n",
      "INFO     root:pipeline_deployment_tests.py:59 Pipeline e2e test artifacts stored in: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/e2e_tests\n",
      "INFO     root:pipeline_deployment_tests.py:67 ML metadata store is ready.\n",
      "=============================== warnings summary ===============================\n",
      "../../../opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22\n",
      "  /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "    import imp\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:696\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:696: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    if not isinstance(type_params, collections.Iterable):\n",
      "\n",
      "../../../opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:535\n",
      "  /opt/conda/lib/python3.7/site-packages/apache_beam/typehints/typehints.py:535: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "    if not isinstance(type_params, (collections.Sequence, set)):\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
      "=========================== short test summary info ============================\n",
      "FAILED src/tests/pipeline_deployment_tests.py::test_e2e_pipeline - AttributeE...\n",
      "======================== 1 failed, 3 warnings in 5.84s"
     ]
    }
   ],
   "source": [
    "!py.test src/tests/pipeline_deployment_tests.py::test_e2e_pipeline -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af62b30",
   "metadata": {},
   "source": [
    "## 2. Run the training pipeline using Vertex Pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da904dc",
   "metadata": {},
   "source": [
    "### Set the pipeline configurations for the Vertex AI run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6134753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATASET_DISPLAY_NAME\"] = DATASET_DISPLAY_NAME\n",
    "os.environ[\"MODEL_DISPLAY_NAME\"] = MODEL_DISPLAY_NAME\n",
    "os.environ[\"PIPELINE_NAME\"] = PIPELINE_NAME\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"GCS_LOCATION\"] = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}\"\n",
    "os.environ[\"TRAIN_LIMIT\"] = \"85000\"\n",
    "os.environ[\"TEST_LIMIT\"] = \"15000\"\n",
    "os.environ[\"BEAM_RUNNER\"] = \"DataflowRunner\"\n",
    "os.environ[\"TRAINING_RUNNER\"] = \"vertex\"\n",
    "os.environ[\"TFX_IMAGE_URI\"] = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ffa2f587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: aiops-industrialization\n",
      "REGION: us-central1\n",
      "GCS_LOCATION: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips\n",
      "ARTIFACT_STORE_URI: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/tfx_artifacts\n",
      "MODEL_REGISTRY_URI: gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/model_registry\n",
      "DATASET_DISPLAY_NAME: chicago-taxi-tips\n",
      "MODEL_DISPLAY_NAME: chicago-taxi-tips-classifier-v01\n",
      "PIPELINE_NAME: chicago-taxi-tips-classifier-v01-train-pipeline\n",
      "ML_USE_COLUMN: ml_use\n",
      "EXCLUDE_COLUMNS: trip_start_timestamp\n",
      "TRAIN_LIMIT: 85000\n",
      "TEST_LIMIT: 15000\n",
      "SERVE_LIMIT: 0\n",
      "NUM_TRAIN_SPLITS: 4\n",
      "NUM_EVAL_SPLITS: 1\n",
      "ACCURACY_THRESHOLD: 0.1\n",
      "USE_KFP_SA: False\n",
      "TFX_IMAGE_URI: gcr.io/aiops-industrialization/chicago-taxi-tips:v01\n",
      "BEAM_RUNNER: DataflowRunner\n",
      "BEAM_DIRECT_PIPELINE_ARGS: ['--project=aiops-industrialization', '--temp_location=gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/temp']\n",
      "BEAM_DATAFLOW_PIPELINE_ARGS: ['--project=aiops-industrialization', '--temp_location=gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/temp', '--region=us-central1', '--runner=DataflowRunner']\n",
      "TRAINING_RUNNER: vertex\n",
      "AI_PLATFORM_TRAINING_ARGS: {'project': 'aiops-industrialization', 'region': 'us-central1', 'masterConfig': {'imageUri': 'gcr.io/aiops-industrialization/chicago-taxi-tips:v01'}}\n",
      "SERVING_RUNTIME: tf2-cpu.2-4\n",
      "SERVING_IMAGE_URI: gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-4:latest\n",
      "BATCH_PREDICTION_BQ_DATASET_NAME: playground_us\n",
      "BATCH_PREDICTION_BQ_TABLE_NAME: chicago_taxitrips_prep\n",
      "BATCH_PREDICTION_BEAM_ARGS: {'runner': 'DataflowRunner', 'temporary_dir': 'gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/temp', 'gcs_location': 'gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/temp', 'project': 'aiops-industrialization', 'region': 'us-central1', 'setup_file': './setup.py'}\n",
      "BATCH_PREDICTION_JOB_RESOURCES: {'machine_type': 'n1-standard-2', 'starting_replica_count': 1, 'max_replica_count': 10}\n",
      "DATASTORE_PREDICTION_KIND: chicago-taxi-tips-classifier-v01-predictions\n",
      "ENABLE_CACHE: 0\n",
      "UPLOAD_MODEL: 0\n"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4eadec",
   "metadata": {},
   "source": [
    "### Build the ML container image\n",
    "\n",
    "This is the `TFX` runtime environment for the training pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "28ab8bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/aiops-industrialization/chicago-taxi-tips:v01\n"
     ]
    }
   ],
   "source": [
    "!echo $TFX_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4540afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 61 file(s) totalling 1.4 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2021.08.31/14.04.16.479943.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://aiops-industrialization_cloudbuild/source/1630418656.561653-a4e8d2ea597a4b0fbad1c8317dca12e3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/aiops-industrialization/locations/global/builds/71160d87-0081-4bb0-80a0-ed8cb3462259].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/71160d87-0081-4bb0-80a0-ed8cb3462259?project=175728527123].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"71160d87-0081-4bb0-80a0-ed8cb3462259\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://aiops-industrialization_cloudbuild/source/1630418656.561653-a4e8d2ea597a4b0fbad1c8317dca12e3.tgz#1630418658660632\n",
      "Copying gs://aiops-industrialization_cloudbuild/source/1630418656.561653-a4e8d2ea597a4b0fbad1c8317dca12e3.tgz#1630418658660632...\n",
      "/ [1 files][315.7 KiB/315.7 KiB]                                                \n",
      "Operation completed over 1 objects/315.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  1.527MB\n",
      "Step 1/5 : FROM gcr.io/tfx-oss-public/tfx:0.30.0\n",
      "0.30.0: Pulling from tfx-oss-public/tfx\n",
      "6e0aa5e7af40: Pulling fs layer\n",
      "d47239a868b3: Pulling fs layer\n",
      "49cbb10cca85: Pulling fs layer\n",
      "4450dd082e0f: Pulling fs layer\n",
      "2c645f69b1d6: Pulling fs layer\n",
      "c51f99ae46a1: Pulling fs layer\n",
      "ece22f1f3a5c: Pulling fs layer\n",
      "0ecbf0b7bbf8: Pulling fs layer\n",
      "5848f1cb3b81: Pulling fs layer\n",
      "2f54b1f16acd: Pulling fs layer\n",
      "16e84ce81d85: Pulling fs layer\n",
      "c8e5550a4fa4: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "8c4a122297af: Pulling fs layer\n",
      "0a3be6fbb919: Pulling fs layer\n",
      "bd37ff43c32f: Pulling fs layer\n",
      "1200922938c4: Pulling fs layer\n",
      "c9544d041b64: Pulling fs layer\n",
      "3ba6aa913af7: Pulling fs layer\n",
      "14f071df3ff5: Pulling fs layer\n",
      "a70527b0d158: Pulling fs layer\n",
      "e83a95442471: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "83550171dabb: Pulling fs layer\n",
      "e7357577046f: Pulling fs layer\n",
      "e39246da14f5: Pulling fs layer\n",
      "792c46c3aca1: Pulling fs layer\n",
      "1200922938c4: Waiting\n",
      "86b48621d244: Pulling fs layer\n",
      "b7db915d224a: Pulling fs layer\n",
      "c9544d041b64: Waiting\n",
      "15be7077a27f: Pulling fs layer\n",
      "5291a275ab91: Pulling fs layer\n",
      "c51f99ae46a1: Waiting\n",
      "5defcd32b48a: Pulling fs layer\n",
      "3ba6aa913af7: Waiting\n",
      "cca8db2402df: Pulling fs layer\n",
      "ece22f1f3a5c: Waiting\n",
      "6e763bd687d6: Pulling fs layer\n",
      "14f071df3ff5: Waiting\n",
      "a7f315d6ed8c: Pulling fs layer\n",
      "01fde12f54d8: Pulling fs layer\n",
      "0ecbf0b7bbf8: Waiting\n",
      "5848f1cb3b81: Waiting\n",
      "a70527b0d158: Waiting\n",
      "e83a95442471: Waiting\n",
      "2f54b1f16acd: Waiting\n",
      "8c4a122297af: Waiting\n",
      "0a3be6fbb919: Waiting\n",
      "bd37ff43c32f: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "16e84ce81d85: Waiting\n",
      "b7db915d224a: Waiting\n",
      "cca8db2402df: Waiting\n",
      "eee173fc570a: Waiting\n",
      "15be7077a27f: Waiting\n",
      "c8e5550a4fa4: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "5291a275ab91: Waiting\n",
      "424e7c9d5d89: Waiting\n",
      "c631c38965fd: Waiting\n",
      "6e763bd687d6: Waiting\n",
      "5defcd32b48a: Waiting\n",
      "83550171dabb: Waiting\n",
      "4450dd082e0f: Waiting\n",
      "a7f315d6ed8c: Waiting\n",
      "e7357577046f: Waiting\n",
      "01fde12f54d8: Waiting\n",
      "86b48621d244: Waiting\n",
      "e39246da14f5: Waiting\n",
      "2c645f69b1d6: Waiting\n",
      "d47239a868b3: Verifying Checksum\n",
      "d47239a868b3: Download complete\n",
      "49cbb10cca85: Verifying Checksum\n",
      "49cbb10cca85: Download complete\n",
      "4450dd082e0f: Verifying Checksum\n",
      "4450dd082e0f: Download complete\n",
      "2c645f69b1d6: Verifying Checksum\n",
      "2c645f69b1d6: Download complete\n",
      "6e0aa5e7af40: Verifying Checksum\n",
      "6e0aa5e7af40: Download complete\n",
      "c51f99ae46a1: Verifying Checksum\n",
      "c51f99ae46a1: Download complete\n",
      "ece22f1f3a5c: Verifying Checksum\n",
      "ece22f1f3a5c: Download complete\n",
      "5848f1cb3b81: Verifying Checksum\n",
      "5848f1cb3b81: Download complete\n",
      "16e84ce81d85: Download complete\n",
      "6e0aa5e7af40: Pull complete\n",
      "d47239a868b3: Pull complete\n",
      "49cbb10cca85: Pull complete\n",
      "4450dd082e0f: Pull complete\n",
      "2c645f69b1d6: Pull complete\n",
      "c51f99ae46a1: Pull complete\n",
      "ece22f1f3a5c: Pull complete\n",
      "2f54b1f16acd: Verifying Checksum\n",
      "2f54b1f16acd: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "c8e5550a4fa4: Verifying Checksum\n",
      "c8e5550a4fa4: Download complete\n",
      "8c4a122297af: Verifying Checksum\n",
      "8c4a122297af: Download complete\n",
      "bd37ff43c32f: Verifying Checksum\n",
      "bd37ff43c32f: Download complete\n",
      "1200922938c4: Verifying Checksum\n",
      "1200922938c4: Download complete\n",
      "0ecbf0b7bbf8: Verifying Checksum\n",
      "0ecbf0b7bbf8: Download complete\n",
      "3ba6aa913af7: Download complete\n",
      "14f071df3ff5: Verifying Checksum\n",
      "14f071df3ff5: Download complete\n",
      "a70527b0d158: Verifying Checksum\n",
      "a70527b0d158: Download complete\n",
      "e83a95442471: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "0a3be6fbb919: Verifying Checksum\n",
      "0a3be6fbb919: Download complete\n",
      "c9544d041b64: Verifying Checksum\n",
      "c9544d041b64: Download complete\n",
      "e39246da14f5: Verifying Checksum\n",
      "e39246da14f5: Download complete\n",
      "83550171dabb: Verifying Checksum\n",
      "83550171dabb: Download complete\n",
      "86b48621d244: Verifying Checksum\n",
      "86b48621d244: Download complete\n",
      "b7db915d224a: Verifying Checksum\n",
      "b7db915d224a: Download complete\n",
      "15be7077a27f: Download complete\n",
      "5291a275ab91: Verifying Checksum\n",
      "5291a275ab91: Download complete\n",
      "5defcd32b48a: Verifying Checksum\n",
      "5defcd32b48a: Download complete\n",
      "cca8db2402df: Verifying Checksum\n",
      "cca8db2402df: Download complete\n",
      "6e763bd687d6: Verifying Checksum\n",
      "6e763bd687d6: Download complete\n",
      "792c46c3aca1: Download complete\n",
      "a7f315d6ed8c: Verifying Checksum\n",
      "a7f315d6ed8c: Download complete\n",
      "01fde12f54d8: Verifying Checksum\n",
      "01fde12f54d8: Download complete\n",
      "e7357577046f: Verifying Checksum\n",
      "e7357577046f: Download complete\n",
      "0ecbf0b7bbf8: Pull complete\n",
      "5848f1cb3b81: Pull complete\n",
      "2f54b1f16acd: Pull complete\n",
      "16e84ce81d85: Pull complete\n",
      "c8e5550a4fa4: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "8c4a122297af: Pull complete\n",
      "0a3be6fbb919: Pull complete\n",
      "bd37ff43c32f: Pull complete\n",
      "1200922938c4: Pull complete\n",
      "c9544d041b64: Pull complete\n",
      "3ba6aa913af7: Pull complete\n",
      "14f071df3ff5: Pull complete\n",
      "a70527b0d158: Pull complete\n",
      "e83a95442471: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "c631c38965fd: Pull complete\n",
      "83550171dabb: Pull complete\n",
      "e7357577046f: Pull complete\n",
      "e39246da14f5: Pull complete\n",
      "792c46c3aca1: Pull complete\n",
      "86b48621d244: Pull complete\n",
      "b7db915d224a: Pull complete\n",
      "15be7077a27f: Pull complete\n",
      "5291a275ab91: Pull complete\n",
      "5defcd32b48a: Pull complete\n",
      "cca8db2402df: Pull complete\n",
      "6e763bd687d6: Pull complete\n",
      "a7f315d6ed8c: Pull complete\n",
      "01fde12f54d8: Pull complete\n",
      "Digest: sha256:4808b9d6cee38c3c7da81657b859f34f49833fbd2b9530341e27bef973a8efe3\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:0.30.0\n",
      " ---> 68adb9229d27\n",
      "Step 2/5 : COPY requirements.txt requirements.txt\n",
      " ---> 0a26dc170b5e\n",
      "Step 3/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in 2df12d92fc17\n",
      "Collecting kfp==1.6.2\n",
      "  Downloading kfp-1.6.2.tar.gz (222 kB)\n",
      "Collecting google-cloud-bigquery==2.20.0\n",
      "  Downloading google_cloud_bigquery-2.20.0-py2.py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage==2.4.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (2.4.0)\n",
      "Collecting google-cloud-aiplatform==1.0.0\n",
      "  Downloading google_cloud_aiplatform-1.0.0-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting google-auth==1.30.1\n",
      "  Downloading google_auth-1.30.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.4 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (0.4.4)\n",
      "Requirement already satisfied: google-auth-httplib2==0.1.0 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.1.0)\n",
      "Requirement already satisfied: oauth2client==4.1.3 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (4.1.3)\n",
      "Requirement already satisfied: requests==2.25.1 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (2.25.1)\n",
      "Collecting pytest\n",
      "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
      "Requirement already satisfied: absl-py<=0.11,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (1.38.0)\n",
      "Requirement already satisfied: kubernetes<13,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (11.0.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (1.12.8)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (1.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.7.0.tar.gz (52 kB)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (0.8.9)\n",
      "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (7.1.2)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.10.tar.gz (20 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (0.1.7)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.2->-r requirements.txt (line 1)) (3.16.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.6.0)\n",
      "Collecting google-api-core[grpc]<2.0.0dev,>=1.29.0\n",
      "  Downloading google_api_core-1.31.2-py2.py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (20.9)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.18.1)\n",
      "Requirement already satisfied: libcst>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-storage==2.4.0->-r requirements.txt (line 3)) (0.3.18)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.30.1->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.30.1->-r requirements.txt (line 5)) (4.2.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.30.1->-r requirements.txt (line 5)) (49.6.0.post20210108)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.30.1->-r requirements.txt (line 5)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth==1.30.1->-r requirements.txt (line 5)) (0.2.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib==0.4.4->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: httplib2>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-httplib2==0.1.0->-r requirements.txt (line 7)) (0.17.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client==4.1.3->-r requirements.txt (line 8)) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests==2.25.1->-r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests==2.25.1->-r requirements.txt (line 9)) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests==2.25.1->-r requirements.txt (line 9)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests==2.25.1->-r requirements.txt (line 9)) (2020.12.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.6.2->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.6.2->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.29.0->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.29.0->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.53.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.29.0->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.32.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.2->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (2.20)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2->-r requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2->-r requirements.txt (line 1)) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.2->-r requirements.txt (line 1)) (0.17.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.2->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.2->-r requirements.txt (line 1)) (0.57.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage==2.4.0->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.2 in /opt/conda/lib/python3.7/site-packages (from libcst>=0.2.5->google-cloud-bigquery-storage==2.4.0->-r requirements.txt (line 3)) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-bigquery==2.20.0->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib==0.4.4->-r requirements.txt (line 6)) (3.0.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.6.2->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from typing-inspect>=0.4.0->libcst>=0.2.5->google-cloud-bigquery-storage==2.4.0->-r requirements.txt (line 3)) (0.4.3)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from pytest->-r requirements.txt (line 10)) (0.10.2)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.10.0-py2.py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.2->-r requirements.txt (line 1)) (3.4.1)\n",
      "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.6.2-py3-none-any.whl size=301630 sha256=f6c5c7f1e25d5f542d6f28fab2e4f424249272cd1e65c68ed7f1202c1a5ee591\n",
      "  Stored in directory: /root/.cache/pip/wheels/ff/cf/15/30d12f6521420ad0ca072497ddccf8e86457c6bf0a2f8a18b2\n",
      "  Building wheel for docstring-parser (PEP 517): started\n",
      "  Building wheel for docstring-parser (PEP 517): finished with status 'done'\n",
      "  Created wheel for docstring-parser: filename=docstring_parser-0.10-py3-none-any.whl size=28862 sha256=34d776d6aebd8ea85ac38635d0f605288efbec8c88118e90208e6434ac1b6bbb\n",
      "  Stored in directory: /root/.cache/pip/wheels/c2/12/f3/67c96229e15e2fe18709cba1b9e3a6a982699c4f0a7b4c0748\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=5142e267ed1b753c86d5c3c5494f813b20f9ef37c8be92eedfca6097ed034284\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.7.0-py3-none-any.whl size=92619 sha256=011b1c2f1c5505c9b4d010df6c72f1903a39662d18c3bb8466e68c885e443d81\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/f0/36/cd1c7475b12b2541f90e4ab9413e59756a11262c1307a97633\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=bc28a0adc304b8a3d5f04a6a1bb58091c459dfee6ef3e446aa23837fe930c7fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: google-auth, google-api-core, strip-hints, requests-toolbelt, py, pluggy, kfp-server-api, iniconfig, google-cloud-bigquery, fire, docstring-parser, Deprecated, pytest, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.30.0\n",
      "    Uninstalling google-auth-1.30.0:\n",
      "      Successfully uninstalled google-auth-1.30.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.26.3\n",
      "    Uninstalling google-api-core-1.26.3:\n",
      "      Successfully uninstalled google-api-core-1.26.3\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 1.28.0\n",
      "    Uninstalling google-cloud-bigquery-1.28.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-1.28.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "Successfully installed Deprecated-1.2.12 docstring-parser-0.10 fire-0.4.0 google-api-core-1.31.2 google-auth-1.30.1 google-cloud-aiplatform-1.0.0 google-cloud-bigquery-2.20.0 iniconfig-1.1.1 kfp-1.6.2 kfp-server-api-1.7.0 pluggy-1.0.0 py-1.10.0 pytest-6.2.5 requests-toolbelt-0.9.1 strip-hints-0.1.10\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.30.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.0.0 which is incompatible.\n",
      "WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "Removing intermediate container 2df12d92fc17\n",
      " ---> 1e8aeced8851\n",
      "Step 4/5 : COPY src/ src/\n",
      " ---> b4d0126844c1\n",
      "Step 5/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      " ---> Running in 7990f1e07ed1\n",
      "Removing intermediate container 7990f1e07ed1\n",
      " ---> 437318ca8e8b\n",
      "Successfully built 437318ca8e8b\n",
      "Successfully tagged gcr.io/aiops-industrialization/chicago-taxi-tips:v01\n",
      "PUSH\n",
      "Pushing gcr.io/aiops-industrialization/chicago-taxi-tips:v01\n",
      "The push refers to repository [gcr.io/aiops-industrialization/chicago-taxi-tips]\n",
      "5cb6f3200bf7: Preparing\n",
      "0a417d7a31eb: Preparing\n",
      "283125d0f174: Preparing\n",
      "533720791ad3: Preparing\n",
      "b450eebf5fd8: Preparing\n",
      "184f8c6d2df6: Preparing\n",
      "2bd9bd3e10a2: Preparing\n",
      "9d6daac4fa86: Preparing\n",
      "b501898dc997: Preparing\n",
      "8003506d39a4: Preparing\n",
      "809d7b2f60d1: Preparing\n",
      "0643eb2eb480: Preparing\n",
      "40a2c3eb7ae8: Preparing\n",
      "79431f0fa8db: Preparing\n",
      "29591854aed3: Preparing\n",
      "d3263f816411: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "f6c5f07e787b: Preparing\n",
      "25fabe96190a: Preparing\n",
      "f5434bbff46e: Preparing\n",
      "a3349cefae00: Preparing\n",
      "17330ad88149: Preparing\n",
      "ae593ab21099: Preparing\n",
      "ac0c09736a4b: Preparing\n",
      "af04e844d06f: Preparing\n",
      "fc6b54c6ced7: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "d5de0a9a6a11: Preparing\n",
      "75867e8b38e6: Preparing\n",
      "edb28f196cf4: Preparing\n",
      "b501898dc997: Waiting\n",
      "463a01dbc7de: Preparing\n",
      "716331d2d72b: Preparing\n",
      "c79fa966f459: Preparing\n",
      "8003506d39a4: Waiting\n",
      "64d8b9e63cdf: Preparing\n",
      "988749f5bf51: Preparing\n",
      "2d4faa2fa9fe: Preparing\n",
      "809d7b2f60d1: Waiting\n",
      "6f15325cc380: Preparing\n",
      "1e77dd81f9fa: Preparing\n",
      "030309cad0ba: Preparing\n",
      "0643eb2eb480: Waiting\n",
      "40a2c3eb7ae8: Waiting\n",
      "d3263f816411: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "79431f0fa8db: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "29591854aed3: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "d5de0a9a6a11: Waiting\n",
      "75867e8b38e6: Waiting\n",
      "edb28f196cf4: Waiting\n",
      "463a01dbc7de: Waiting\n",
      "716331d2d72b: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "c79fa966f459: Waiting\n",
      "f6c5f07e787b: Waiting\n",
      "ae593ab21099: Waiting\n",
      "64d8b9e63cdf: Waiting\n",
      "25fabe96190a: Waiting\n",
      "988749f5bf51: Waiting\n",
      "f5434bbff46e: Waiting\n",
      "ac0c09736a4b: Waiting\n",
      "2d4faa2fa9fe: Waiting\n",
      "a3349cefae00: Waiting\n",
      "6f15325cc380: Waiting\n",
      "af04e844d06f: Waiting\n",
      "17330ad88149: Waiting\n",
      "1e77dd81f9fa: Waiting\n",
      "fc6b54c6ced7: Waiting\n",
      "030309cad0ba: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "184f8c6d2df6: Waiting\n",
      "2bd9bd3e10a2: Waiting\n",
      "9d6daac4fa86: Waiting\n",
      "b450eebf5fd8: Layer already exists\n",
      "533720791ad3: Layer already exists\n",
      "184f8c6d2df6: Layer already exists\n",
      "2bd9bd3e10a2: Layer already exists\n",
      "9d6daac4fa86: Layer already exists\n",
      "8003506d39a4: Layer already exists\n",
      "809d7b2f60d1: Layer already exists\n",
      "0643eb2eb480: Layer already exists\n",
      "b501898dc997: Layer already exists\n",
      "40a2c3eb7ae8: Layer already exists\n",
      "79431f0fa8db: Layer already exists\n",
      "29591854aed3: Layer already exists\n",
      "06a5bf49b163: Layer already exists\n",
      "b34dae69fc5d: Layer already exists\n",
      "d3263f816411: Layer already exists\n",
      "0ffb7465dde9: Layer already exists\n",
      "e2563d1ada9a: Layer already exists\n",
      "25fabe96190a: Layer already exists\n",
      "f6c5f07e787b: Layer already exists\n",
      "f5434bbff46e: Layer already exists\n",
      "a3349cefae00: Layer already exists\n",
      "17330ad88149: Layer already exists\n",
      "ac0c09736a4b: Layer already exists\n",
      "ae593ab21099: Layer already exists\n",
      "fc6b54c6ced7: Layer already exists\n",
      "af04e844d06f: Layer already exists\n",
      "5b9e34b5cf74: Layer already exists\n",
      "d5de0a9a6a11: Layer already exists\n",
      "75867e8b38e6: Layer already exists\n",
      "463a01dbc7de: Layer already exists\n",
      "716331d2d72b: Layer already exists\n",
      "c79fa966f459: Layer already exists\n",
      "64d8b9e63cdf: Layer already exists\n",
      "988749f5bf51: Layer already exists\n",
      "2d4faa2fa9fe: Layer already exists\n",
      "283125d0f174: Pushed\n",
      "6f15325cc380: Layer already exists\n",
      "1e77dd81f9fa: Layer already exists\n",
      "030309cad0ba: Layer already exists\n",
      "5cb6f3200bf7: Pushed\n",
      "0a417d7a31eb: Pushed\n",
      "edb28f196cf4: Mounted from tfx-oss-public/tfx\n",
      "v01: digest: sha256:c7fd159756ed58ef9a58a470bd17415dfaf76ec14b350667d91dd69a27b71cc0 size: 9136\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                 IMAGES                                                STATUS\n",
      "71160d87-0081-4bb0-80a0-ed8cb3462259  2021-08-31T14:04:19+00:00  4M20S     gs://aiops-industrialization_cloudbuild/source/1630418656.561653-a4e8d2ea597a4b0fbad1c8317dca12e3.tgz  gcr.io/aiops-industrialization/chicago-taxi-tips:v01  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "00b59549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import (\n",
    "    StatisticsGen,\n",
    "    ExampleValidator,\n",
    "    Transform,\n",
    "    Trainer,\n",
    "    Evaluator,\n",
    "    Pusher,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f47eef",
   "metadata": {},
   "source": [
    "### Compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0cc0f27d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'statistics_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18923/4155295002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipeline_definition_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{config.PIPELINE_NAME}.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(pipeline_definition_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpipeline_definition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_training_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_definition_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/mlops-with-vertex-ai/src/tfx_pipelines/runner.py\u001b[0m in \u001b[0;36mcompile_training_pipeline\u001b[0;34m(pipeline_definition_file)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hidden_units\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIDDEN_UNITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mptype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         ),\n\u001b[1;32m     56\u001b[0m     )\n",
      "\u001b[0;32m~/mlops-with-vertex-ai/src/tfx_pipelines/training_pipeline.py\u001b[0m in \u001b[0;36mcreate_pipeline\u001b[0;34m(pipeline_root, num_epochs, batch_size, learning_rate, hidden_units, metadata_connection_config)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# Example validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     example_validator = ExampleValidator(\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;31m#statistics=statistics_gen.outputs.statistics,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mstatistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatistics_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statistics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema_importer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'statistics_gen' is not defined"
     ]
    }
   ],
   "source": [
    "from src.tfx_pipelines import runner\n",
    "\n",
    "pipeline_definition_file = f'{config.PIPELINE_NAME}.json'\n",
    "#print(pipeline_definition_file)\n",
    "pipeline_definition = runner.compile_training_pipeline(pipeline_definition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e4abbcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://aiops-industrialization-bucket-ravi/chicago-taxi-tips/compiled_pipelines/\n"
     ]
    }
   ],
   "source": [
    "PIPELINES_STORE = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/compiled_pipelines/\"\n",
    "print(PIPELINES_STORE)\n",
    "#!gsutil cp {pipeline_definition_file} {PIPELINES_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ca754",
   "metadata": {},
   "source": [
    "### Submit run to Vertex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "030b3746",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chicago-taxi-tips-classifier-v01-train-pipeline.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18923/978758695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m'hidden_units'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'128,128'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py\u001b[0m in \u001b[0;36mcreate_run_from_job_spec\u001b[0;34m(self, job_spec_path, job_id, pipeline_root, parameter_values, enable_caching, cmek, service_account, network, labels)\u001b[0m\n\u001b[1;32m    306\u001b[0m       \u001b[0;32mor\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \"\"\"\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mjob_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_spec_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0mpipeline_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pipelineSpec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pipelineInfo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     job_id = job_id or '{pipeline_name}-{timestamp}'.format(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/v2/google/client/client_utils.py\u001b[0m in \u001b[0;36mload_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_load_json_from_gs_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_load_json_from_local_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/v2/google/client/client_utils.py\u001b[0m in \u001b[0;36m_load_json_from_local_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOn\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mparsing\u001b[0m \u001b[0mproblems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chicago-taxi-tips-classifier-v01-train-pipeline.json'"
     ]
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "pipeline_client = AIPlatformClient(\n",
    "    project_id=PROJECT, region=REGION)\n",
    "                 \n",
    "job = pipeline_client.create_run_from_job_spec(\n",
    "    job_spec_path=pipeline_definition_file,\n",
    "    parameter_values={\n",
    "        'learning_rate': 0.003,\n",
    "        'batch_size': 512,\n",
    "        'hidden_units': '128,128',\n",
    "        'num_epochs': 30,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d6391",
   "metadata": {},
   "source": [
    "### Extracting pipeline runs metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace8969",
   "metadata": {},
   "source": [
    "## 3. Execute the pipeline deployment CI/CD steps in Cloud Build\n",
    "\n",
    "The CI/CD routine is defined in the [pipeline-deployment.yaml](pipeline-deployment.yaml) file, and consists of the following steps:\n",
    "1. Clone the repository to the build environment.\n",
    "2. Run unit tests.\n",
    "3. Run a local e2e test of the pipeline.\n",
    "4. Build the ML container image for pipeline steps.\n",
    "5. Compile the pipeline.\n",
    "6. Upload the pipeline to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3f2fb",
   "metadata": {},
   "source": [
    "### Build CI/CD container Image for Cloud Build\n",
    "\n",
    "This is the runtime environment where the steps of testing and deploying the pipeline will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CICD_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a090aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $CICD_IMAGE_URI build/. --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606e6e6",
   "metadata": {},
   "source": [
    "### Run CI/CD from pipeline deployment using Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/ksalama/ucaip-labs.git\" # Change to your github repo.\n",
    "BRANCH = \"main\"\n",
    "\n",
    "GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/\"\n",
    "TEST_GCS_LOCATION = f\"gs://{BUCKET}/{DATASET_DISPLAY_NAME}/e2e_tests\"\n",
    "CI_TRAIN_LIMIT = 1000\n",
    "CI_TEST_LIMIT = 100\n",
    "CI_UPLOAD_MODEL = 0\n",
    "CI_ACCURACY_THRESHOLD = 0.1\n",
    "BEAM_RUNNER = \"DataflowRunner\"\n",
    "TRAINING_RUNNER = \"vertex\"\n",
    "VERSION = 'tfx-0-30'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "PIPELINES_STORE = os.path.join(GCS_LOCATION, \"compiled_pipelines\")\n",
    "\n",
    "TFX_IMAGE_URI = f\"gcr.io/{PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "\n",
    "SUBSTITUTIONS=f\"\"\"\\\n",
    "_REPO_URL='{REPO_URL}',\\\n",
    "_BRANCH={BRANCH},\\\n",
    "_CICD_IMAGE_URI={CICD_IMAGE_URI},\\\n",
    "_PROJECT={PROJECT},\\\n",
    "_REGION={REGION},\\\n",
    "_GCS_LOCATION={GCS_LOCATION},\\\n",
    "_TEST_GCS_LOCATION={TEST_GCS_LOCATION},\\\n",
    "_BQ_LOCATION={BQ_LOCATION},\\\n",
    "_BQ_DATASET_NAME={BQ_DATASET_NAME},\\\n",
    "_BQ_TABLE_NAME={BQ_TABLE_NAME},\\\n",
    "_DATASET_DISPLAY_NAME={DATASET_DISPLAY_NAME},\\\n",
    "_MODEL_DISPLAY_NAME={MODEL_DISPLAY_NAME},\\\n",
    "_CI_TRAIN_LIMIT={CI_TRAIN_LIMIT},\\\n",
    "_CI_TEST_LIMIT={CI_TEST_LIMIT},\\\n",
    "_CI_UPLOAD_MODEL={CI_UPLOAD_MODEL},\\\n",
    "_CI_ACCURACY_THRESHOLD={CI_ACCURACY_THRESHOLD},\\\n",
    "_BEAM_RUNNER={BEAM_RUNNER},\\\n",
    "_TRAINING_RUNNER={TRAINING_RUNNER},\\\n",
    "_TFX_IMAGE_URI={TFX_IMAGE_URI},\\\n",
    "_PIPELINE_NAME={PIPELINE_NAME},\\\n",
    "_PIPELINES_STORE={PIPELINES_STORE}\\\n",
    "\"\"\"\n",
    "\n",
    "!echo $SUBSTITUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47870343",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --no-source --timeout=60m --config build/pipeline-deployment.yaml --substitutions {SUBSTITUTIONS} --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0bdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
